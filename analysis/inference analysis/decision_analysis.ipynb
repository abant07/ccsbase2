{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# XGBoost Model Decision Analysis\n",
    "\n",
    "This notebook analyzes the trained XGBoost model for CCS prediction:\n",
    "- Feature importance analysis\n",
    "- Tree structure visualization\n",
    "- SHAP value analysis for comprehensive feature impact understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from xgboost import plot_tree, plot_importance\n",
    "import shap\n",
    "\n",
    "from utils import Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-model-header",
   "metadata": {},
   "source": [
    "## 1. Load Model and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load-model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "Number of estimators: 6000\n",
      "Max depth: 10\n",
      "Learning rate: 0.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/pickle.py:1835: UserWarning: [15:41:16] WARNING: /Users/runner/work/xgboost/xgboost/src/gbm/../common/error_msg.h:83: If you are loading a serialized model (like pickle in Python, RDS in R) or\n",
      "configuration generated by an older version of XGBoost, please export the model by calling\n",
      "`Booster.save_model` from that version first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html\n",
      "\n",
      "for more details about differences between saving model and serializing.\n",
      "\n",
      "  setstate(state)\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model = joblib.load('ccsbase2.joblib')\n",
    "print(f\"Model loaded successfully\")\n",
    "print(f\"Number of estimators: {model.n_estimators}\")\n",
    "print(f\"Max depth: {model.max_depth}\")\n",
    "print(f\"Learning rate: {model.learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "get-adducts",
   "metadata": {},
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql 'SELECT adduct FROM master_clean GROUP BY adduct HAVING COUNT(*) >= 100 ORDER BY adduct': no such table: master_clean",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/pandas/io/sql.py:2664\u001b[39m, in \u001b[36mSQLiteDatabase.execute\u001b[39m\u001b[34m(self, sql, params)\u001b[39m\n\u001b[32m   2663\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2664\u001b[39m     \u001b[43mcur\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2665\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cur\n",
      "\u001b[31mOperationalError\u001b[39m: no such table: master_clean",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mDatabaseError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m conn = sqlite3.connect(database_file)\n\u001b[32m      4\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mSELECT adduct FROM master_clean GROUP BY adduct HAVING COUNT(*) >= 100 ORDER BY adduct\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m adducts = \u001b[38;5;28msorted\u001b[39m(\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_sql_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m.to_numpy().tolist())\n\u001b[32m      6\u001b[39m adducts = [adduct[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m adduct \u001b[38;5;129;01min\u001b[39;00m adducts]\n\u001b[32m      7\u001b[39m conn.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/pandas/io/sql.py:528\u001b[39m, in \u001b[36mread_sql_query\u001b[39m\u001b[34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize, dtype, dtype_backend)\u001b[39m\n\u001b[32m    525\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m dtype_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib.no_default\n\u001b[32m    527\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[32m--> \u001b[39m\u001b[32m528\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m        \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    536\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    537\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/pandas/io/sql.py:2728\u001b[39m, in \u001b[36mSQLiteDatabase.read_query\u001b[39m\u001b[34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[39m\n\u001b[32m   2717\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_query\u001b[39m(\n\u001b[32m   2718\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2719\u001b[39m     sql,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2726\u001b[39m     dtype_backend: DtypeBackend | Literal[\u001b[33m\"\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2727\u001b[39m ) -> DataFrame | Iterator[DataFrame]:\n\u001b[32m-> \u001b[39m\u001b[32m2728\u001b[39m     cursor = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2729\u001b[39m     columns = [col_desc[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col_desc \u001b[38;5;129;01min\u001b[39;00m cursor.description]\n\u001b[32m   2731\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/pandas/io/sql.py:2676\u001b[39m, in \u001b[36mSQLiteDatabase.execute\u001b[39m\u001b[34m(self, sql, params)\u001b[39m\n\u001b[32m   2673\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minner_exc\u001b[39;00m\n\u001b[32m   2675\u001b[39m ex = DatabaseError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExecution failed on sql \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2676\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mDatabaseError\u001b[39m: Execution failed on sql 'SELECT adduct FROM master_clean GROUP BY adduct HAVING COUNT(*) >= 100 ORDER BY adduct': no such table: master_clean"
     ]
    }
   ],
   "source": [
    "# Get the adduct list (same as used during training)\n",
    "database_file = '../../ccs.db'\n",
    "conn = sqlite3.connect(database_file)\n",
    "query = \"SELECT adduct FROM master_clean GROUP BY adduct HAVING COUNT(*) >= 100 ORDER BY adduct\"\n",
    "adducts = sorted(pd.read_sql_query(query, conn).to_numpy().tolist())\n",
    "adducts = [adduct[0] for adduct in adducts]\n",
    "conn.close()\n",
    "\n",
    "print(f\"Number of adducts: {len(adducts)}\")\n",
    "print(f\"Adducts: {adducts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "create-feature-names",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adducts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m feature_names = [\u001b[33m'\u001b[39m\u001b[33mMolecularWeight\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mAdductMass\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mCharge\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mLabuteASA\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Add adduct one-hot encoding names\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m adduct \u001b[38;5;129;01min\u001b[39;00m \u001b[43madducts\u001b[49m:\n\u001b[32m      6\u001b[39m     feature_names.append(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mAdduct_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00madduct\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m      7\u001b[39m feature_names.append(\u001b[33m'\u001b[39m\u001b[33mAdduct_Other\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'adducts' is not defined"
     ]
    }
   ],
   "source": [
    "# Create feature names\n",
    "feature_names = ['MolecularWeight', 'AdductMass', 'Charge', 'LabuteASA']\n",
    "\n",
    "# Add adduct one-hot encoding names\n",
    "for adduct in adducts:\n",
    "    feature_names.append(f'Adduct_{adduct}')\n",
    "feature_names.append('Adduct_Other')\n",
    "\n",
    "# Add Morgan fingerprint bits\n",
    "for i in range(1024):\n",
    "    feature_names.append(f'MorganFP_{i}')\n",
    "\n",
    "print(f\"Total features: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "load-test-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data shape: (9749, 8)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'adducts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     10\u001b[39m valid_indices = []\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m test_df.iterrows():\n\u001b[32m     13\u001b[39m     feat_values = utils.calculate_descriptors(\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         row[\u001b[33m'\u001b[39m\u001b[33msmi\u001b[39m\u001b[33m'\u001b[39m], row[\u001b[33m'\u001b[39m\u001b[33mmass\u001b[39m\u001b[33m'\u001b[39m], row[\u001b[33m'\u001b[39m\u001b[33mz\u001b[39m\u001b[33m'\u001b[39m], \u001b[43madducts\u001b[49m, row[\u001b[33m'\u001b[39m\u001b[33madduct\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     15\u001b[39m     )\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m feat_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     17\u001b[39m         X_list.append(feat_values)\n",
      "\u001b[31mNameError\u001b[39m: name 'adducts' is not defined"
     ]
    }
   ],
   "source": [
    "# Load test data for SHAP analysis\n",
    "test_df = pd.read_csv('../../pretrained/test_data.csv')\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "\n",
    "utils = Utils()\n",
    "\n",
    "# Calculate features for test data\n",
    "X_list = []\n",
    "y_list = []\n",
    "valid_indices = []\n",
    "\n",
    "for idx, row in test_df.iterrows():\n",
    "    feat_values = utils.calculate_descriptors(\n",
    "        row['smi'], row['mass'], row['z'], adducts, row['adduct']\n",
    "    )\n",
    "    if feat_values is not None:\n",
    "        X_list.append(feat_values)\n",
    "        y_list.append(row['ccs'])\n",
    "        valid_indices.append(idx)\n",
    "\n",
    "X_test = np.array(X_list)\n",
    "y_test = np.array(y_list)\n",
    "\n",
    "print(f\"Valid test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-importance-header",
   "metadata": {},
   "source": [
    "## 2. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "builtin-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances from model\n",
    "importance_types = ['weight', 'gain', 'cover']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for ax, imp_type in zip(axes, importance_types):\n",
    "    importance = model.get_booster().get_score(importance_type=imp_type)\n",
    "    \n",
    "    # Convert to dataframe and sort\n",
    "    imp_df = pd.DataFrame({\n",
    "        'feature': list(importance.keys()),\n",
    "        'importance': list(importance.values())\n",
    "    }).sort_values('importance', ascending=True).tail(20)\n",
    "    \n",
    "    # Map feature indices to names\n",
    "    imp_df['feature_name'] = imp_df['feature'].apply(\n",
    "        lambda x: feature_names[int(x[1:])] if x.startswith('f') else x\n",
    "    )\n",
    "    \n",
    "    ax.barh(imp_df['feature_name'], imp_df['importance'])\n",
    "    ax.set_xlabel(f'Importance ({imp_type})')\n",
    "    ax.set_title(f'Top 20 Features by {imp_type.capitalize()}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "importance-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive importance table\n",
    "all_features = set()\n",
    "for imp_type in importance_types:\n",
    "    importance = model.get_booster().get_score(importance_type=imp_type)\n",
    "    all_features.update(importance.keys())\n",
    "\n",
    "importance_data = []\n",
    "for feat in all_features:\n",
    "    feat_idx = int(feat[1:]) if feat.startswith('f') else -1\n",
    "    feat_name = feature_names[feat_idx] if feat_idx >= 0 and feat_idx < len(feature_names) else feat\n",
    "    \n",
    "    row_data = {'feature': feat, 'feature_name': feat_name}\n",
    "    for imp_type in importance_types:\n",
    "        importance = model.get_booster().get_score(importance_type=imp_type)\n",
    "        row_data[imp_type] = importance.get(feat, 0)\n",
    "    importance_data.append(row_data)\n",
    "\n",
    "importance_df = pd.DataFrame(importance_data)\n",
    "importance_df = importance_df.sort_values('gain', ascending=False)\n",
    "\n",
    "print(\"Top 30 Features by Gain:\")\n",
    "importance_df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tree-vis-header",
   "metadata": {},
   "source": [
    "## 3. Tree Structure Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-single-tree",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a single tree (first tree)\n",
    "fig, ax = plt.subplots(figsize=(30, 15))\n",
    "plot_tree(model, num_trees=0, ax=ax, rankdir='TB')\n",
    "plt.title('Tree 0 Structure')\n",
    "plt.savefig('tree_0.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-multiple-trees",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot first 5 trees for comparison\n",
    "for tree_idx in range(min(5, model.n_estimators)):\n",
    "    fig, ax = plt.subplots(figsize=(25, 12))\n",
    "    plot_tree(model, num_trees=tree_idx, ax=ax, rankdir='TB')\n",
    "    plt.title(f'Tree {tree_idx} Structure')\n",
    "    plt.savefig(f'tree_{tree_idx}.png', dpi=80, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tree-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tree statistics\n",
    "booster = model.get_booster()\n",
    "trees_df = booster.trees_to_dataframe()\n",
    "\n",
    "print(f\"Total nodes across all trees: {len(trees_df)}\")\n",
    "print(f\"\\nNode types:\")\n",
    "print(trees_df['Feature'].value_counts().head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tree-depth-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze tree depth distribution\n",
    "depth_stats = trees_df.groupby('Tree')['Depth'].max()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.hist(depth_stats, bins=20, edgecolor='black')\n",
    "ax.set_xlabel('Max Tree Depth')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Distribution of Tree Depths')\n",
    "plt.savefig('tree_depth_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean max depth: {depth_stats.mean():.2f}\")\n",
    "print(f\"Std max depth: {depth_stats.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shap-header",
   "metadata": {},
   "source": [
    "## 4. SHAP Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shap-explainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SHAP explainer\n",
    "explainer = shap.TreeExplainer(model)\n",
    "\n",
    "# Use a sample for faster computation (adjust size as needed)\n",
    "sample_size = min(1000, len(X_test))\n",
    "np.random.seed(42)\n",
    "sample_indices = np.random.choice(len(X_test), sample_size, replace=False)\n",
    "X_sample = X_test[sample_indices]\n",
    "\n",
    "print(f\"Computing SHAP values for {sample_size} samples...\")\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "print(\"SHAP values computed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shap-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Summary Plot (bar)\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values, X_sample, feature_names=feature_names, \n",
    "                  plot_type='bar', max_display=30, show=False)\n",
    "plt.title('SHAP Feature Importance (Mean |SHAP|)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('shap_importance_bar.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shap-beeswarm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Beeswarm Plot (shows direction of feature impact)\n",
    "plt.figure(figsize=(12, 10))\n",
    "shap.summary_plot(shap_values, X_sample, feature_names=feature_names, \n",
    "                  max_display=30, show=False)\n",
    "plt.title('SHAP Summary (Beeswarm)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('shap_beeswarm.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shap-dependence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Dependence Plots for top features\n",
    "# Find top features by mean absolute SHAP\n",
    "mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
    "top_feature_indices = np.argsort(mean_abs_shap)[-6:][::-1]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feat_idx in enumerate(top_feature_indices):\n",
    "    shap.dependence_plot(feat_idx, shap_values, X_sample, \n",
    "                         feature_names=feature_names, ax=axes[i], show=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('shap_dependence_top6.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shap-waterfall",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Waterfall plot for a single prediction\n",
    "# Show how each feature contributes to a specific prediction\n",
    "sample_idx = 0\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.waterfall_plot(shap.Explanation(\n",
    "    values=shap_values[sample_idx], \n",
    "    base_values=explainer.expected_value,\n",
    "    data=X_sample[sample_idx],\n",
    "    feature_names=feature_names\n",
    "), max_display=20, show=False)\n",
    "plt.title(f'SHAP Waterfall for Sample {sample_idx}')\n",
    "plt.tight_layout()\n",
    "plt.savefig('shap_waterfall_example.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shap-force",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force plot for a few samples\n",
    "shap.initjs()\n",
    "\n",
    "# Single prediction force plot\n",
    "shap.force_plot(explainer.expected_value, shap_values[0], X_sample[0], \n",
    "                feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grouped-analysis-header",
   "metadata": {},
   "source": [
    "## 5. Grouped Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grouped-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group features and analyze importance by category\n",
    "feature_groups = {\n",
    "    'Molecular Properties': [0, 1, 2, 3],  # MW, AdductMass, Charge, LabuteASA\n",
    "    'Adduct Encoding': list(range(4, 4 + len(adducts) + 1)),\n",
    "    'Morgan Fingerprints': list(range(4 + len(adducts) + 1, len(feature_names)))\n",
    "}\n",
    "\n",
    "group_importance = {}\n",
    "for group_name, indices in feature_groups.items():\n",
    "    group_shap = np.abs(shap_values[:, indices]).mean()\n",
    "    group_importance[group_name] = group_shap\n",
    "\n",
    "# Plot grouped importance\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "groups = list(group_importance.keys())\n",
    "values = list(group_importance.values())\n",
    "\n",
    "bars = ax.bar(groups, values, color=['#2ecc71', '#3498db', '#9b59b6'])\n",
    "ax.set_ylabel('Mean |SHAP Value|')\n",
    "ax.set_title('Feature Group Importance')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, values):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "            f'{val:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('grouped_feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFeature Group Importance:\")\n",
    "for group, imp in group_importance.items():\n",
    "    print(f\"  {group}: {imp:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "top-fingerprints",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze top Morgan fingerprint bits\n",
    "fp_start = 4 + len(adducts) + 1\n",
    "fp_shap = np.abs(shap_values[:, fp_start:]).mean(axis=0)\n",
    "\n",
    "top_fp_indices = np.argsort(fp_shap)[-20:][::-1]\n",
    "\n",
    "print(\"Top 20 Morgan Fingerprint Bits by SHAP Importance:\")\n",
    "for i, idx in enumerate(top_fp_indices):\n",
    "    print(f\"  {i+1}. Bit {idx}: {fp_shap[idx]:.4f}\")\n",
    "\n",
    "# Plot top fingerprint bits\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.bar(range(20), [fp_shap[i] for i in top_fp_indices])\n",
    "ax.set_xticks(range(20))\n",
    "ax.set_xticklabels([f'Bit {i}' for i in top_fp_indices], rotation=45, ha='right')\n",
    "ax.set_ylabel('Mean |SHAP Value|')\n",
    "ax.set_title('Top 20 Morgan Fingerprint Bits')\n",
    "plt.tight_layout()\n",
    "plt.savefig('top_fingerprint_bits.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interaction-header",
   "metadata": {},
   "source": [
    "## 6. Feature Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shap-interaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SHAP interaction values (computationally expensive)\n",
    "# Using a smaller sample for interaction analysis\n",
    "interaction_sample_size = min(200, len(X_sample))\n",
    "X_interaction = X_sample[:interaction_sample_size]\n",
    "\n",
    "print(f\"Computing SHAP interaction values for {interaction_sample_size} samples...\")\n",
    "print(\"(This may take a while)\")\n",
    "\n",
    "# Focus on top features only for interaction analysis\n",
    "top_n = 10\n",
    "top_indices = np.argsort(mean_abs_shap)[-top_n:][::-1]\n",
    "\n",
    "# Get interaction values for the full model\n",
    "shap_interaction = explainer.shap_interaction_values(X_interaction)\n",
    "print(\"Interaction values computed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interaction-heatmap",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction heatmap for top features\n",
    "interaction_matrix = np.abs(shap_interaction[:, top_indices, :][:, :, top_indices]).mean(axis=0)\n",
    "\n",
    "top_feature_names = [feature_names[i] for i in top_indices]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "im = ax.imshow(interaction_matrix, cmap='YlOrRd')\n",
    "ax.set_xticks(range(len(top_indices)))\n",
    "ax.set_yticks(range(len(top_indices)))\n",
    "ax.set_xticklabels(top_feature_names, rotation=45, ha='right')\n",
    "ax.set_yticklabels(top_feature_names)\n",
    "plt.colorbar(im, label='Mean |Interaction Value|')\n",
    "ax.set_title('SHAP Interaction Values (Top 10 Features)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('shap_interaction_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## 7. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report\n",
    "print(\"=\" * 60)\n",
    "print(\"XGBoost Model Analysis Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  - Number of trees: {model.n_estimators}\")\n",
    "print(f\"  - Max depth: {model.max_depth}\")\n",
    "print(f\"  - Learning rate: {model.learning_rate}\")\n",
    "print(f\"  - Total features: {len(feature_names)}\")\n",
    "\n",
    "print(f\"\\nTop 10 Most Important Features (by SHAP):\")\n",
    "top_10_indices = np.argsort(mean_abs_shap)[-10:][::-1]\n",
    "for i, idx in enumerate(top_10_indices):\n",
    "    print(f\"  {i+1}. {feature_names[idx]}: {mean_abs_shap[idx]:.4f}\")\n",
    "\n",
    "print(f\"\\nFeature Group Contributions:\")\n",
    "total_importance = sum(group_importance.values())\n",
    "for group, imp in group_importance.items():\n",
    "    pct = (imp / total_importance) * 100\n",
    "    print(f\"  - {group}: {pct:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
